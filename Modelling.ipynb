{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the modelling process went:\n",
    "\n",
    "Model | Train | Validation | Kaggle\n",
    ":--- | :--- | :--- | :---\n",
    "first logistic regression | 0.5929 | 0.5930 | 0.5950\n",
    "first RF (100 trees) | 0.8560 | 0.7582 | 0.7743\n",
    "first XGB (lr 0.2) | 0.8076 | 0.7768 | 0.7573\n",
    "RF (300 trees) | 0.8567 | 0.7590 |\n",
    "XGB (lr 0.05) | 0.8165 | 0.7802 | 0.7769 (top 67%)\n",
    "-|-|-|-\n",
    "smaller RF + imputation | 0.8557 | 0.7603 | \n",
    "smaller XGB + imputation | 0.8084 | 0.7790 | \n",
    "bigger RF + imputation | 0.8569 | 0.7611 | \n",
    "bigger XGB + imputation | 0.8137 | 0.7818 | 0.7743\n",
    "-|-|-|-\n",
    "smaller RF + interactions | 0.8656 | 0.7689 | \n",
    "smaller XGB + interactions | 0.8047 | 0.7764 | \n",
    "bigger RF + interactions | 0.8664 | 0.7695 | 0.7668\n",
    "bigger XGB + interactions | 0.8132 | 0.7799 | 0.7770 (top 67%)\n",
    "-|-|-|-\n",
    "smaller RF + selection | 0.8578 | 0.7705 | \n",
    "smaller XGB + selection | 0.8045 | 0.7768 | 0.7742\n",
    "bigger RF + selection | 0.8588 | 0.7710 | 0.7682\n",
    "bigger XGB + selection | 0.8107 | 0.7803 | 0.7770\n",
    "-|-|-|-\n",
    "big RF tuned | 0.8502 | 0.7726 | 0.7702\n",
    "big XGB tuned | 0.7931 | 0.7799 | 0.7778 (top 67%)\n",
    "-|-|-|-\n",
    "very big RF, tuned, all data | 0.8497 | - | 0.7708\n",
    "very big XGB, tuned, all data | 0.7918 | - | 0.7783 (top 66%)\n",
    "\n",
    "Tree-based ensemble models immediately outperformed logistic regression. In my experience XGBoost also almost always outperforms Random Forest and XGB can be improved more with tuning but I will try both for a while. A bigger XGB puts us in the top 67%. Validation AUC seems to be a good approximation of the public test set's AUC, but we need to take into account that a single validation set is not as reliable as crossvalidation. Crossvalidation is sadly currently inconvenient because to prevent overfitting and retain validation AUC as a decent estimate of leaderboard AUC, many features such as target encodings need to be created on train set only. \n",
    "\n",
    "Using sklearn.impute.IterativeImputer instead of random distibution sampling improved validation results a tiny bit but not the leaderboard score. Because it is almost certainly better than random values, I will count the kaggle score not improving as noise due the fact that it is calculated on only 25% of total test data. \n",
    "\n",
    "Adding >1000 feature products on top of that yielded very slightly better results on RF, but not on XGB (except for the most minor improvent on the leaderboard). Most features were not useful at all as shown by feature importances graphs so for both model goodness and time-efficiency we should try to pick out 150-200 features from the 1100+. Selecting a subset of features improved RF and XGB slightly so it was likely beneficial. Tuning and using all data obviously improved results, but as with previous changes, not nearly as much as I was expecting even though the optimal parameters were pretty different from the ones I used in the beginning. \n",
    "\n",
    "This was a very curious dataset, the task seemed very straightforward initially but i guess that was the reason why the leaderboard was incredibly densely packed and missing value imputation, feature interactions and tuning barely improved the end result. \n",
    "\n",
    "\n",
    "### Things still possible to try:\n",
    "\n",
    "different sampling methods for uneven class distributions\n",
    "\n",
    "multiple different models on differently engineered datasets, boosted together\n",
    "\n",
    "different types and sizes of feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "develop = False\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "figsize = (6,14)\n",
    "\n",
    "def plot_importances(model, error=False, title=''):\n",
    "    \n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)#[::-1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    if error:\n",
    "        std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "        plt.barh(range(train_X.shape[1]), importances[indices], xerr=std[indices], align=\"center\")\n",
    "    else:\n",
    "        plt.barh(range(train_X.shape[1]), importances[indices], align=\"center\")\n",
    "\n",
    "    plt.yticks(range(train_X.shape[1]), [train_X.columns[ix] for ix in indices], rotation='horizontal')\n",
    "    plt.ylim([-1, train_X.shape[1]])\n",
    "    ax.xaxis.tick_top()\n",
    "    plt.title(title, y=1.03)\n",
    "    plt.show()\n",
    "\n",
    "def plot_xgboost_importances(booster, model_name):\n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    plot_importance(booster=booster, ax=ax, height=0.8, title=model_name+' - uses', importance_type='weight')\n",
    "    plt.show()\n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    plot_importance(booster=booster, ax=ax, height=0.8, title=model_name+' - avg gain', importance_type='gain')\n",
    "    plt.show()\n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    plot_importance(booster=booster, ax=ax, height=0.8, title=model_name+' - avg affected samples', \n",
    "                    importance_type='cover')\n",
    "    plt.show()\n",
    "\n",
    "def plot_tree_usefulness(model, validation_X, validation_y, model_name):\n",
    "    predictions = []\n",
    "    for tree in model.estimators_:\n",
    "        predictions.append(tree.predict(validation_X))\n",
    "        \n",
    "    predictions = np.vstack(predictions)\n",
    "    predictions = np.cumsum(predictions, axis=0)\n",
    "    predictions = [predictions[i]/(i+1) for i in range(len(predictions))] #cum mean\n",
    "\n",
    "    scores = []\n",
    "    for pred in predictions:\n",
    "        scores.append(AUC(validation_y, pred))\n",
    "        \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(scores, linewidth=3)\n",
    "    plt.xlabel('trees')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.title(model_name+' tree usefulnesses')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 600000 entries, 0 to 599999\n",
      "Columns: 190 entries, ord_2_x_ord_3 to id\n",
      "dtypes: float16(134), float32(8), int16(10), int32(1), int8(37)\n",
      "memory usage: 206.6 MB\n"
     ]
    }
   ],
   "source": [
    "# all features with big XGB model importance measure > 0.0012 (188/1136)\n",
    "best_features = ['ord_2_x_ord_3', 'ord_0_x_ord_3', 'bin_0', 'ord_0_x_ord_2', 'ord_3_x_ord_5', 'ord_5_x_month', \n",
    "'bin_2_x_month', 'ord_0_x_month', 'ord_1_x_month', 'ord_0_x_ord_5', 'ord_2_x_month', 'ord_0_x_ord_4', \n",
    "'day_x_x_nom_1_Triangle', 'ord_2_x_ord_5', 'bin_2_x_ord_4', 'nom_1_target_mean', 'month_x_nom_4_Bassoon', \n",
    "'nom_8_target_mean', 'bin_0_target_mean', 'nom_3_target_mean', 'nom_9_target_mean', 'nom_7_target_mean', \n",
    "'ord_1_x_ord_3', 'ord_4_x_ord_5', 'ord_1_x_ord_4', 'day_target_mean', 'day_x_x_nom_3_India', 'bin_0_x_nom_3_lat', \n",
    "'bin_0_x_ord_5', 'ord_2', 'bin_1', 'ord_1_x_ord_2', 'month_x_nom_1_Trapezoid', 'bin_2_x_ord_3', \n",
    "'ord_4_x_nom_3_Russia', 'nom_5_target_mean', 'month_x_nom_3_Russia', 'ord_2_x_nom_4_wind', 'bin_2_x_ord_2', \n",
    "'nom_2_target_mean', 'month_target_mean', 'nom_1_Triangle', 'ord_4_x_nom_4_Bassoon', 'ord_0_x_ord_1', \n",
    "'bin_2_x_ord_5', 'bin_2_x_ord_0', 'ord_2_target_mean', 'ord_4_x_day_x', 'bin_2_x_ord_1', 'ord_3_x_nom_4_wind', \n",
    "'month_x_day_x', 'bin_2_x_nom_4_Bassoon', 'nom_4_Piano', 'ord_0_x_nom_4_Piano', 'ord_4_x_nom_1_Trapezoid', \n",
    "'bin_0_x_month_x', 'ord_3_x_nom_1_Trapezoid', 'nom_6_target_mean', 'ord_0_x_nom_4_Bassoon', 'bin_4_x_nom_4_Bassoon', \n",
    "'nom_4_target_mean', 'ord_3', 'ord_2_x_nom_1_Trapezoid', 'ord_3_x_nom_4_Bassoon', 'ord_3_x_nom_3_Russia', \n",
    "'bin_4_x_month', 'bin_2_x_bin_4', 'bin_0_x_month', 'ord_3_x_day_x', 'bin_1_x_day', 'day_x_x_nom_4_Bassoon', \n",
    "'ord_2_x_day_x', 'ord_2_x_nom_4_Bassoon', 'ord_4_x_nom_2_Lion', 'nom_0_Blue_x_nom_3_India', 'month_x_nom_2_Lion', \n",
    "'ord_0_x_nom_4_wind', 'nom_4_wind_x_nom_3_Russia', 'month_x_x_nom_0_Red', 'nom_8_was_na', 'ord_1_x_ord_5', \n",
    "'bin_1_x_month_x', 'bin_4_x_nom_0_Blue', 'ord_1_x_nom_2_Lion', 'nom_4_wind_x_nom_2_Lion', 'bin_0_x_ord_3', \n",
    "'ord_5_target_mean', 'ord_0_x_day_x', 'bin_4_x_ord_3', 'bin_1_x_nom_4_wind', 'bin_4_x_nom_3_Russia', \n",
    "'nom_0_Blue_x_nom_2_Lion', 'nom_1_Polygon_x_nom_4_Bassoon', 'bin_2_x_day_x', 'bin_4_x_day_x', 'ord_5_x_nom_3_Russia', \n",
    "'ord_0', 'ord_1_x_nom_4_Bassoon', 'bin_2_was_na', 'ord_4_x_nom_0_Blue', 'ord_2_x_nom_4_Piano', 'day_x_day_y', \n",
    "'ord_1_x_day_x', 'bin_2_x_nom_1_Trapezoid', 'ord_5_x_nom_4_Piano', 'bin_2_x_nom_0_Blue', 'ord_3_target_mean', \n",
    "'month_x_nom_0_Blue', 'bin_1_x_nom_4_Theremin', 'ord_0_x_nom_3_India', 'ord_1_x_nom_1_Trapezoid', \n",
    "'ord_5_x_nom_1_Trapezoid', 'ord_4_x_day_y', 'bin_4_x_ord_4', 'bin_0_x_ord_0', 'ord_4_target_mean', \n",
    "'ord_0_x_nom_2_Lion', 'day_x_x_nom_0_Blue', 'bin_2_x_nom_3_lat', 'ord_3_x_nom_4_Piano', 'ord_3_x_nom_0_Blue', \n",
    "'nom_9_was_na', 'ord_4_x_nom_4_wind', 'nom_3_Russia_x_nom_4_Bassoon', 'day_x_x_nom_1_Trapezoid', \n",
    "'day_y_x_nom_1_Triangle', 'day_y_x_nom_4_Bassoon', 'ord_3_was_na', 'ord_5_x_nom_4_Bassoon', 'bin_0_x_day', \n",
    "'bin_4_x_ord_1', 'day_x_x_nom_3_Russia', 'nom_1_Star_x_nom_4_Theremin', 'ord_2_x_nom_2_Lion', 'bin_4_x_nom_4_wind', \n",
    "'bin_0_x_ord_4', 'bin_2_x_nom_2_Lion', 'bin_1_x_ord_4', 'ord_2_x_nom_1_Star', 'month_y', 'ord_3_x_nom_2_Lion', \n",
    "'bin_4_x_ord_2', 'bin_1_x_ord_5', 'ord_2_x_ord_4', 'nom_1_Trapezoid_x_nom_4_Bassoon', 'nom_7_was_na', \n",
    "'month_x_x_nom_3_Finland', 'nom_0_Red_x_nom_4_Theremin', 'day_x_nom_1_Trapezoid', 'day_x_nom_4_Piano', \n",
    "'month_y_x_nom_0_Blue', 'nom_5_was_na', 'month_was_na', 'nom_3_Costa Rica_x_nom_4_Bassoon', \n",
    "'nom_3_lat_x_nom_1_Triangle', 'bin_3_x_nom_0_Red', 'bin_2_x_nom_3_Russia', 'day_x_nom_4_Bassoon', \n",
    "'ord_5_x_nom_3_Costa Rica', 'month_x_nom_4_Piano', 'day_x_x_nom_3_Costa Rica', 'ord_0_x_nom_2_Axolotl', \n",
    "'month_x_nom_2_Axolotl', 'nom_1_Trapezoid_x_nom_3_Russia', 'day_x_nom_2_Axolotl', 'ord_4_x_nom_1_Triangle', \n",
    "'ord_5_x_nom_3_China', 'nom_0_Red_x_nom_1_Circle', 'ord_4_x_nom_1_Polygon', 'ord_2_was_na', 'ord_5_x_day_x', \n",
    "'ord_5_x_nom_3_Finland', 'nom_4_wind_x_month_y', 'bin_1_x_nom_3_India', 'bin_2_x_nom_4_wind', 'ord_0_x_nom_1_Polygon', \n",
    "'month_x_day_y', 'month_y_x_nom_0_Red', 'ord_5', 'day_x_nom_1_Star', 'ord_4_x_nom_2_Axolotl', 'ord_0_x_nom_3_Russia', \n",
    "'bin_3_x_nom_2_Hamster', 'ord_2_x_nom_3_Russia', 'day_x_x_nom_2_Axolotl', 'month_x_nom_0_Green', \n",
    "'nom_0_Green_x_nom_2_Axolotl', 'ord_3_x_day_y']\n",
    "\n",
    "if develop:\n",
    "    train = pd.read_pickle('data/train.p')[best_features + ['target', 'id']]#.sample(frac=0.001)\n",
    "else:\n",
    "    train = pd.read_pickle('data/all.p')[best_features + ['target', 'id']]#.sample(frac=0.001)\n",
    "\n",
    "validation = pd.read_pickle('data/validation.p')[best_features + ['target', 'id']]\n",
    "test = pd.read_pickle('data/test.p')[best_features + ['id']]\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "presets = [\n",
    "    {\n",
    "        'model_name' : 'rf',\n",
    "        'model_class' : RandomForestRegressor,\n",
    "        'model_params' : {\n",
    "            'n_jobs': -1,\n",
    "            'n_estimators': 1000,\n",
    "            \n",
    "            'max_features': 0.25,\n",
    "            'max_samples': 0.8,\n",
    "            \n",
    "            'max_depth': None,\n",
    "            'min_samples_leaf': 50\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'model_name' : 'xgb',\n",
    "        'model_class' : XGBRegressor,\n",
    "        'model_params' : {\n",
    "            'n_jobs': -1,\n",
    "            'n_estimators': 3500,\n",
    "            \n",
    "            'learning_rate': 0.025,\n",
    "            'max_depth': 1,\n",
    "            'min_child_weight': 2500,\n",
    "            \n",
    "            'subsample': 0.85,\n",
    "            'colsample_bytree': 0.55\n",
    "        },\n",
    "        'fit_params' : {\n",
    "            #'early_stopping_rounds': 50,\n",
    "            'eval_metric': 'auc'\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: rf\n",
      "train ROC AUC: 0.849737992399993\n",
      "validation ROC AUC: 0.8178828572130334\n",
      "\n",
      "Model: xgb\n",
      "train ROC AUC: 0.7918165984281322\n",
      "validation ROC AUC: 0.7809126656786327\n",
      "\n",
      "Wall time: 2h 21min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_X = train.drop('target', axis=1)\n",
    "train_y = train['target']\n",
    "del train\n",
    "validation_X = validation.drop('target', axis=1)\n",
    "validation_y = validation['target']\n",
    "del validation\n",
    "predict_X = test\n",
    "del test\n",
    "\n",
    "for preset in presets:\n",
    "    \n",
    "    model_name = preset['model_name']\n",
    "    model_class = preset['model_class']\n",
    "    model_params = preset['model_params']\n",
    "    \n",
    "    print('Model:', model_name)\n",
    "    model = model_class(**model_params)\n",
    "    \n",
    "    if model_class == XGBRegressor:\n",
    "        fit_params = preset['fit_params']\n",
    "        if 'early_stopping_rounds' in fit_params.keys():\n",
    "            eval_set = [(train_X, train_y), (validation_X, validation_y)]\n",
    "            model.fit(train_X, train_y, eval_set=eval_set, verbose=False, **fit_params)\n",
    "        else: \n",
    "            model.fit(train_X, train_y, verbose=False, **fit_params)\n",
    "    else:\n",
    "        model.fit(train_X, train_y)\n",
    "    \n",
    "    preset['model'] = model\n",
    "    \n",
    "    if 'n_iter_no_change' in model_params.keys():\n",
    "        print('Model contains', len(model.estimators_), 'trees')\n",
    "    if model_class == XGBRegressor:\n",
    "        if 'early_stopping_rounds' in fit_params.keys():\n",
    "            print('Model contains', model.best_iteration+1, 'trees')\n",
    "            ntree_limit = model.best_ntree_limit\n",
    "        else:\n",
    "            ntree_limit = model_params['n_estimators']\n",
    "    \n",
    "    if model_class == XGBRegressor:\n",
    "        train_prediction = model.predict(train_X, ntree_limit=ntree_limit)\n",
    "        validation_prediction = model.predict(validation_X, ntree_limit=ntree_limit)\n",
    "    elif model_class == LogisticRegression:\n",
    "        train_prediction = model.predict_proba(train_X)\n",
    "        train_prediction = [x[1] for x in train_prediction]\n",
    "        validation_prediction = model.predict_proba(validation_X)\n",
    "        validation_prediction = [x[1] for x in validation_prediction]\n",
    "    else:\n",
    "        train_prediction = model.predict(train_X)\n",
    "        validation_prediction = model.predict(validation_X)\n",
    "\n",
    "    print('train ROC AUC: {}'.format(AUC(train_y, train_prediction)))\n",
    "    print('validation ROC AUC: {}'.format(AUC(validation_y, validation_prediction)))\n",
    "    \n",
    "    if develop:\n",
    "        \n",
    "        if model_class == XGBRegressor:\n",
    "            plot_importances(model, error=False, title=model_name)\n",
    "            plot_xgboost_importances(model, model_name)\n",
    "\n",
    "        if model_class == RandomForestRegressor:\n",
    "            plot_importances(model, error=True, title=model_name)\n",
    "            plot_tree_usefulness(model, validation_X, validation_y, model_name)\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/submission_rf.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andreas\\Anaconda3\\envs\\kaggle\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/submission_xgb.csv\n"
     ]
    }
   ],
   "source": [
    "for i, preset in enumerate(presets):\n",
    "    \n",
    "    model_class = preset['model_class']\n",
    "    model_name = preset['model_name']\n",
    "    model = preset['model']\n",
    "    \n",
    "    filename = 'data/submission_{}.csv'.format(model_name)\n",
    "    print(filename)\n",
    "    \n",
    "    if model_class == XGBRegressor:\n",
    "        if 'early_stopping_rounds' in fit_params.keys():\n",
    "            ntree_limit = model.best_ntree_limit\n",
    "        else:\n",
    "            ntree_limit = model_params['n_estimators']\n",
    "        final_prediction = model.predict(predict_X, ntree_limit=ntree_limit)\n",
    "    elif model_class == LogisticRegression:\n",
    "        final_prediction = model.predict_proba(predict_X)\n",
    "        final_prediction = [x[1] for x in final_prediction]\n",
    "    else:\n",
    "        final_prediction = model.predict(predict_X)\n",
    "    final_prediction = np.clip(final_prediction, 0, 1)\n",
    "\n",
    "    prediction = predict_X[['id']]\n",
    "    prediction['target'] = final_prediction\n",
    "    prediction.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model = presets[1]['model']\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "features = pd.DataFrame({\n",
    "    'feature': [train_X.columns[ix] for ix in indices],  \n",
    "    'importance': [importances[ix] for ix in indices]})\n",
    "\n",
    "qs = 1 - np.array([0.05, 0.1, 0.15, 0.2])\n",
    "print(features['importance'].quantile(qs))\n",
    "\n",
    "features.hist(bins=60, figsize=(18, 4))\n",
    "plt.xticks(np.arange(0, 0.03, 0.001))\n",
    "plt.show()\n",
    "\n",
    "features[features['importance'] < 0.003].hist(bins=30, figsize=(18, 4))\n",
    "plt.xticks(np.arange(0, 0.004, 0.0002))\n",
    "plt.show()\n",
    "\n",
    "subset = features[features['importance'] > 0.0012]\n",
    "print(len(subset))\n",
    "print(subset['feature'].tolist())\n",
    "\"\"\";"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
